---
title: "NHANES"
author: "RG"
date: "2023-06-26"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library("splines")
library("DT")
library("isotree")
```

## Load the data

There are 6063 observations, some are incomplete and have missing values for some covariates. There are 22 covariates, which have cryptic names and you need to use the meta-data to resolve them.  The survey is very complex and typically any analysis requires a substantial amount of reading of the documentation.  Here we will guide you past some of the hurdles.

We load up the data and the metadata. In the metadata we have a textual description of the phenotype, the short name, and the target.  The target tells us which of the sampled individuals was eligible to answer the question. 

```{r loadData}
nhanesDataPath = ""
load(paste0(nhanesDataPath, "d4.rda"))
load(paste0(nhanesDataPath, "metaD.rda")) 
DT::datatable(metaD)
```

We will look at the relationship between the variable LBXTC (which is Total Cholesterol in mg/dL measured by a blood draw) and the age of the participant in years. 

```{r, echo=FALSE}
plot(d4$RIDAGEYR, d4$LBXTC, xlab="Age in Years", ylab="Total Cholesterol, mg/dL")
```

And we can see that plotting, over-plotting is a substantial issue here.
You might also notice what seems like a lot of data at age 80, this is because any age over 80 was truncated to 80 to prevent reidentification of survey participants. In a complete analysis, this should probably be adjusted for in some way, but we will ignore it for now.

We can try some other methods, such as `hexbin` plotting and `smoothScatter`

```{r, echo=FALSE}
smoothScatter(d4$RIDAGEYR, d4$LBXTC, xlab="Age in Years", ylab="Total Cholesterol, mg/dL")
```

Now we can see a few outliers - with extremely high serum cholesterol.
We get a sense that the trend is not exactly a straight line, but rather a parabola, lower for the young and the old and a bit higher in the middle.

We fit a linear model first.

```{r}    
lm1 = lm(d4$LBXTC ~ d4$RIDAGEYR)
summary(lm1)
```

And we can plot the data together with the fitted regression line.
The green line is horizontal and at the mean value of `LBXTC`, while the blue line is the regression line.  You can see that even though the data are parabolic in shape, the regression line slopes slightly upwards.  Predictions from this model will be somewhat problematic.
The data don't seem to agree with a straight-line model.

```{r plot1, echo=FALSE}
plot(d4$RIDAGEYR, d4$LBXTC)
abline(lm1, col="blue", lwd=2)
abline(h=mean(d4$LBXTC,na.rm=TRUE), col="green", lwd=2)
```

Next we can look at a plot of the residuals against the fitted values.  In the code we use a scatter plot smoother called `lowess` to estimate the smooth trend in our residuals, which is plotted in blue.  The red line is horizontal at 0. And we can see that the residuals are more negative for high and low values in the fitted values and they are more positive for middle values.

```{r, echo=TRUE}
plot(lm1$fitted.values, lm1$residuals)
##fit a loess curve
l2 = lowess(lm1$residuals ~ lm1$fitted.values)
lines(l2, col="blue", lwd=2)
abline(h=0, col="red", lwd=2)

```

Notice that both terms in the model are very significant, but that the multiple $R^2$ is only around 2%.  So age, in years, is not explaining very much of the variation. But because we have such a large data set, the parameter estimates are found to be significantly different from zero.

## Spline Models

- when a linear model does not appear sufficient we can try other models.
- one choice is to use natural splines, which are very flexible
- they are based on B-splines with the previso that the model is linear outside the range of the data
- based on the initial analysis, we chose to use df=7, which gives five internal knots when fitting the splines
- you have almost 6,000 degrees of freedom here, so using up a few to get a more appropriate fit seems good.

```{r}
library("splines")
lm2 = lm(d4$LBXTC ~ ns(d4$RIDAGEYR, df=7))
summary(lm2)
```


- we can use standard tools for comparing models
- the `anova` function takes as input two (or more) models and compares them
- the method requires that the datasets be the same, and if there are missing values in some covariates then this will not be true
- in the example below we see that the improvement in fit with the spline model is quite substantial

```{r}
anova(lm1, lm2)
```

Notice also that the multiple $R^2$ went up to about 10%, a pretty substantial increase, suggesting that the curvilinear nature of the relationship is substantial.

The residual standard error also decreased by about 5%.

We have lost the simple explanation that comes from fitting a linear model. We cannot say that your serum cholesterol increases by $a$ units per year, but that model was wrong, so it really shouldn't be used.

**Exercise**
Plot the residuals for `lm2` against the fitted values. Add a horizontal line at 0 and describe whether you see any nonlinearity?
**End**

We can use the regression model we fit to make predictions for any one, and these are substantially more accurate.


## Spline Models

- even though the regression summary prints out a different row for each spline term, they are not independent variables, and you need to either retain them all, or retain none of them

- you can use `predict` methods for spline models to get either confidence or prediction intervals

## Sex

- next we might want to start to add other variables and explore the different relationships.
- let's consider sex, for now we will leave out age, and just try to understand what happens with sex
- first I will fit the model without an intercept

```{r}
lm3 = lm(LBXTC ~ RIAGENDR-1, data=d4)
summary(lm3)

```

- here we can see the mean for males is a bit higher than for females
- both are significant and notice how large the multiple $R^2$ value is
- this is not a very interesting test - we are asking if the mean is zero, which isn't even physically possible
- our model is
$$
  y_i = \beta_M \cdot 1_{M,i} + \beta_F \cdot 1_{F,i}
$$
- where $1_{M,i}$ is 1 if the $i^{th}$ case is male and zero otherwise, similarly for $1_{F,i}$

- instead we ask if the mean for males is different than that for females
$$ 
  y_i = \beta_0 + \beta_1 \cdot 1_{F,i}
$$

- so that $E[Y|M] = \beta_0$ and $E[Y|F] = \beta_0 + \beta_1$
- $\beta_1$ estimates the difference in mean between male and female


```{r}
lm3 = lm(LBXTC ~ RIAGENDR, data=d4)
summary(lm3)

```

- now we see an Intercept term (that will be the overall mean)
- and the estimate for females is represents how they differ, if it is zero then there is no difference in total cholesterol between men and women

## Multivariate Regression

- now we will put together a set of features (variables) that we are interested in
- for simplicity we only keep participants for which we have all the data
- because we have removed cases (individuals) these models are not directly comparable with the ones we fit above.


```{r}
ivars = c("RIDAGEYR", "RIAGENDR", "RIDRETH1", "DMDEDUC2", "INDFMPIR", "LBDHDD", "LBXGH", "BMXBMI", "LBXTC")

d4sub = d4[,ivars]
compCases = apply(d4sub, 1, function(x) sum(is.na(x)))
cC = compCases==0
d4sub = d4sub[cC,]
dim(d4sub)

```

##One quick transformation

- the variable `DMDEDUC2` is a bit too granular for our purposes
- we will modify it to be, less than high school, high school and more than high school

```{r}
table(d4sub$DMDEDUC2)
dd = d4sub$DMDEDUC2

dd[dd=="Don't Know"] = NA

eduS = ifelse(dd == "Less than 9th grade" | dd =="9-11th grade (Includes 12th grade with no diploma)", "<HS", ifelse(dd == "High school graduate/GED or equivalent", "HS", ">HS" ))

#stick this into our dataframe
#and drop the NA
d4sub$eduS = eduS
d4sub = d4sub[-which(is.na(eduS)), ]

table(eduS, dd, useNA = "always")
##now drop the DMDEDUC2 variable
d4sub$DMDEDUC2 = NULL
```

## Principal Components

- we can take the continuous variables and look at principle components
- plotting the first two PCs suggest that these directions are dominated by outliers and that a good step in our analysis might be to remove them
```{r}
cvars = c("RIDAGEYR", "INDFMPIR", "LBDHDD", "LBXGH", "BMXBMI", "LBXTC")
contd4sub=d4sub[, cvars]
##keep a version around so we can look for outliers later
oldd4 = d4sub

pcs = prcomp(contd4sub)

##based on pc plot we have at least 3 outliers that are dominating the first 2 pcs
##check to see if they are extreme - we see the two very large LBXTC values as well as one other value that is high for LBXTC and very high for LBDHDD
d4sub[c(1077, 2876, 2933),]


contd4sub = contd4sub[-c(1077, 2876, 2933),]
d4sub = d4sub[-c(1077, 2876, 2933),]
pcs = prcomp(contd4sub)

pcvals=pcs$x

##which(abs(pcs$x[,1]) > 300)
##which(abs(pcs$x[,2]) > 100)
```

We can look at the rotation matrix.  It will tells us a bit about the directions in the multivariate data that are associated with variation.

```{r pcarotation}
pcs$rotation

```
Now, looking at the column PC1, we see that there is a very large value for LBXTC and in fact this direction is almost
all about that variable.  For PC2, we see large positive values (the sign is not important, the fact that they are both positive says
this is sort of an average of the two) for RIDAGEYR and LBDHDD.  PC3 is a contrast, RIDAGEYR is nexative and LBDHDD is positive, so
we will get large values when one of the two is small (close to zero) and the the other is large. The last two PCs are mostly INDFMPIR and LBXGH, respectively.

What is interesting is that LBXTC, BMXBMI, INDFMPIR and LBXGH are largely orthogonal to each other.
While RIDAGEYR and LBDHDD have some more interesting relationships. Note that men tend to have lower values
of HDL than women, which we see in the plot.

```{r plotvars}
plot(d4sub$RIDAGEYR, d4sub$LBDHDD, col=ifelse(d4sub$RIAGENDR=="Male", "coral1", "seagreen" ))

```
Now let's look in PCA space.  We want to see what the plot of PC2 against PC3.
I don't have a very good idea of why this mean/difference relationship exists in the data.

```{r pcaplot}
plot(pcvals[,2], pcvals[,3], col=ifelse(d4sub$RIAGENDR=="Male", "coral1", "seagreen" ), xlab="PC2", ylab="PC3")

```


## Random Forests

- Random Forests are a simple way to get a sense of how important different variables are in predicting a variable of interest.
- we will cover Random Forests in some detail in our AI/ML lecture for now we will just apply them

```{r}
library("randomForest")
rf1 = randomForest(LBXTC ~ ., proximity=TRUE, data=d4sub)
varImpPlot(rf1)
```


## Back to Regression

```{r}
lmF = lm(LBXTC ~ ., data=d4sub)
summary(lmF)
```

 We see that being Non-hispanic black seems to have a pretty big effect, so we might want to just include that, and group all other ethnicities together
 Education level seems to have little to add, we can drop those.
 
It might be good to get a sense of the relationship with the poverty level variable.

```{r}
 Black = ifelse(d4sub$RIDRETH1 == "Non-Hispanic Black", "B", "nonB")
 ivars = c("RIDAGEYR", "INDFMPIR", "LBDHDD", "LBXGH", "BMXBMI", "LBXTC", "eduS", "RIAGENDR")

 d5sub = cbind(d4sub[,ivars], Black)
 lmFx = lm(LBXTC ~ ns(RIDAGEYR,df=7) + INDFMPIR + LBDHDD + LBXGH + BMXBMI + eduS + RIAGENDR + Black , data=d5sub)
 summary(lmFx)
 
```
 
Now refit the model with the terms for sex and educational status removed.

```{r small-model, eval=TRUE, echo=TRUE}

lmFxSm = lm(LBXTC ~ ns(RIDAGEYR,df=7) + INDFMPIR + LBDHDD + LBXGH + BMXBMI + Black , data=d5sub)
 summary(lmFxSm)

```
**Exercise:**
Plot RIDAGEYR versus INDFMPIR.
What do you see in the plot?  Does anything cause you concern about fitting a linear model?
```{r plotAGEvPoverty, echo=TRUE, eval=FALSE}
plot( d5sub$INDFMPIR, d5sub$RIDAGEYR)
```


## Missing Indicator Approach

The missing indicator approach may be useful for data where there is a limit of detection, so that values below some lower bound $\alpha_L$ or above some upper bound $\alpha_U$ are set to $\alpha_L$ or to $\alpha_U$ respectively.  A similar approach can be used for the Windsorization used for RIDAGEYR variable, where values over 80 are set to 80.  We are not proposing using this for other types of missingness, although there is a righ literature and users may want to explore the broader use of this method.  However, it is important to realize that often bias or increased variance may obtain, often dependent on untestable assumptions.

The reason that we believe this approach is appropriate for the cases listed is that we actually did measure the variable, and many over variables on those individuals, but we cannot report the exact value for any individual.  Hence, one can interpret the indicator as being some form of average over all individuals affected by the reporting limits.  It is probably worth noting that these limitations have implications for predication methods. While one in principle could estimate some outcome for an 88 year old person, the data won't support that prediction.  Instead one should ignore the linear predictor for age and use the indicator variable.
Chiou *et al* (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6812630/) provide some rationale for logistic regression and the references therein point to results for linear regression. Groenwold *et al* (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3414599/) provide advice on more general use of the method to address missing data.


Next let's try to fix out model to deal with the repeated values in these two variables.  Now an important consideration is to try and assess just how to interpret them.  For RIDAGEYR the documentation states that anyone over age 80 in the database has their age represented as 80.  This is not censoring.  The measurements (eg BMI, cholesterol etc.) were all made on a person of some age larger than 80.  We just Windsorized their ages, and so these individuals really are not the same as the others, where we are getting accurate age values. 

So basically what we want to do is to create a *dummy variable* for those whose age is reported as 80.

```{r missingIndicator, echo=TRUE, eval=TRUE}

Age80 = d5sub$RIDAGEYR == 80
d6sub=cbind(d5sub, Age80)

lmFx2  = lm(LBXTC ~ ns(RIDAGEYR,df=7) + INDFMPIR + LBDHDD + LBXGH + BMXBMI + Black + Age80 , data=d6sub)
 summary(lmFx2)

```
**Exercise:**
What changes do you notice in the model fit?
Use the anova function to compare `lmFx3` to `lmFx2`.
How do you interpret the output?
<look at INDFMPIR>

**Exercise:**
Try to fit missing indicator variables for both of the repeated values in the INDFMPIR variable.
Then interpret the output.

```{r Poverty, echo=TRUE, eval=TRUE}
Pov5 = d6sub$INDFMPIR == 5
Pov0 = d6sub$INDFMPIR == 0
d7sub = cbind(d6sub, Pov5, Pov0)

lmFx2  = lm(LBXTC ~ ns(RIDAGEYR,df=7) + INDFMPIR + LBDHDD + LBXGH + BMXBMI + Black + Age80 + Pov0 + Pov5, data=d7sub)
 summary(lmFx2)

```


*Exercise::*

In the code below, we drop the terms that were not statistically significant in the model and then compare this smaller model to the larger one, above.
Interpret the results.
```{r}
lmFx4 = lm(LBXTC ~ ns(RIDAGEYR, df=7) + LBDHDD+LBXGH+Black, data=d7sub)
summary(lmFx4)

anova(lmFx4, lmFx2)
```